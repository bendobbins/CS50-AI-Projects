First, I tried implementing the program in the same way as the handwriting.py source code from the lecture to get a feel for how the neural network worked. That gave me around a 0.05 accuracy when testing, so I knew I had to change some things around. I tried adding second convolution and pooling layers, which drastically increased the accuracy up to around 0.95 when testing. It worked so well that I tried adding third convolution and pooling layers, but that sent the accuracy back down to around 0.05 again, so I removed those. I then tried changing the pool size for each of the layers a few times, but none turned out better than the first run, so I left both pool sizes at 2x2. After that, I tried messing with the number of convolution filters and the size, but none of that improved the accuracy so I moved on to the hidden layers.
I messed with the layers and dropout for about an hour or two, adding new hidden layers of different sizes and adjusting dropout values for each layer, but the best accuracy I was able to achieve was around 0.96, which is pretty underwhelming seeing as I got around 0.95 on my second try. I know it was never going to be perfect because there will always be some error, but I was hoping to get around 0.98 or 0.99. Oh well. I ended up using 2 hidden layers with descending numbers and dropout values. I tried a lot of different combinations with 1 hidden layer and 3 hidden layers as well, but none were able to consistently break the 0.97 threshold. It seemed like adding too many hidden layers was a detriment, but leaving too few also left the program falling somewhat short. I noted as well that the time it took to complete increased as the number in each hidden layer was increased, which makes sense.